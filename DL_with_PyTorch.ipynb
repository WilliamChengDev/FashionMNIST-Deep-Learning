{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2209402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "#input transformation seqeunce\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((28, 28)), #resize input to 28x28\n",
    "        transforms.ToTensor(), #convert image to pytorch tensor [0, 1] range\n",
    "        transforms.Normalize((0.5, ), (0.5, )) #normalize values to [-1, 1] range\n",
    "])\n",
    "\n",
    "#load traning and test FashionMNIST dataset\n",
    "'''\n",
    "root --> save location if download is needed#train --> what portion of the dataset to load (training or testing)\n",
    "download --> download dataset if not present in root\n",
    "transform --> apply transitions to each image as it is being accessed\n",
    "'''\n",
    "full_train_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059b13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_ratio):\n",
    "        #Split training dataset into training and validation sets (80%/20%)\n",
    "        train_size = int(train_ratio * len(full_train_dataset))\n",
    "        val_size = len(full_train_dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "        #Create DataLoaders\n",
    "        '''\n",
    "        batch_size --> each batch retreived by the loader will get 512 samples at once\n",
    "                - reduces gradient noise\n",
    "                - allows parallel computing by GPU\n",
    "                - forward pass --> CEL --> backward pass --> Gradient update\n",
    "        shuffle --> shuffle order samples are loaded\n",
    "                - shuffle for training set to prevent overfitting\n",
    "                - no shuffle for validation and testing set to ensure consistent & reproducible evaluation\n",
    "        num_workers --> how many parallel processes will be handling data loading\n",
    "        '''\n",
    "        train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=2)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=2)\n",
    "\n",
    "        print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "246aae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNISTNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FashionMNISTNet(nn.Module):\n",
    "        '''\n",
    "        input: 28x28\n",
    "        conv1 = 32x28x28\n",
    "        maxpool = 32x14x14\n",
    "        conv2 = 64x16x16\n",
    "        maxpool = 64x8x8\n",
    "        conv3 = 64x8x8\n",
    "        maxpool = 64x4x4\n",
    "        flatten = 1024\n",
    "        '''\n",
    "        def __init__(self):\n",
    "                super(FashionMNISTNet, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "                self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2)\n",
    "                self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "                self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                self.fc1 = nn.Linear(in_features=1024, out_features=256)\n",
    "                self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "                self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "        def forward(self, x):\n",
    "                x = self.pool(F.relu(self.conv1(x)))\n",
    "                x = self.pool(F.relu(self.conv2(x)))\n",
    "                x = self.pool(F.relu(self.conv3(x)))\n",
    "                x = torch.flatten(x, start_dim=1)\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = F.relu(self.fc2(x))\n",
    "                x = self.fc3(x) #no ReLU on the last fully connected layer\n",
    "                # x = F.softmax(x, dim=1)\n",
    "                return x\n",
    "        \n",
    "net = FashionMNISTNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31e09595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "        #Test\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "\n",
    "                        # Move the inputs and labels to the GPU if available\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        # Forward pass\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                        # Get the predicted class\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                        # Update the total number of samples and correct predictions\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cac7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(model, train_loader, val_loader, device, criterion, optimizer):\n",
    "        # Training loop\n",
    "        num_epochs = 15\n",
    "        for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        running_loss += loss.item()\n",
    "\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "\n",
    "                # ---validation---\n",
    "                model.eval()\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():  \n",
    "                        for inputs, labels in val_loader:\n",
    "                                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                                outputs = model(inputs)\n",
    "                                _, predicted = torch.max(outputs.data, 1)\n",
    "                                total += labels.size(0)\n",
    "                                correct += (predicted == labels).sum().item()\n",
    "                if (total == 0):\n",
    "                        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "                else:\n",
    "                        val_acc = 100 * correct / total\n",
    "                        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca1b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBelow - Training with differing validation set sizes:\\n\\nThe accuracy of the different validation sets are as following:\\n0% validation set: 87.28%\\n10% validation set: 86.78%\\n20% validation set: 86.57%\\n30% validation set: 78.45%\\n40% validation set: 84.71%\\n\\nOne general trend between acuraccy and the size of the validation set was that as the size of \\nthe validation set increased, the accuracy of the trained model decreased. Doing further research\\nI learned that the decrease in accuracy is likely due to the fact that there is less data for the \\nmodel to train on when more data is allocated for validation within the full training set. The \\ndifference in accuracy from 0%-20% was minimal, but took a big dip from 20%-30% validation, likely\\nindicating that the model underfit when 30% of the full training set was used for validation, as \\nthe model did not have enough data to work with.\\n\\nInitially, one confusing part was the rebound in accuracy at 40%, however I believe this is due to the fact that\\nthe training/validation split was made randomly, and therefore the model might have been trained on \\ndiffering data that resulting in a normal fluctuation of the accuracy, although the general trend \\nremains. It might be difficult to get a consistent evaluation of these due to the fact that the size \\nand contents of the training set are hard to normalize due to the difference in size, although the validation\\nset should be more accurate the larger the validation set is, due to there being more data used to \\nvalidate the model during training.\\n\\nAll in all, the size of the validation set is a balance between the risk of underfitting vs validation accuracy/\\nper epoch training time. The larger the validation set, the more accurate the validation, but the smaller the training \\nset, which results in shorter training time, but potentially a less accurate model. Although if the database is large\\nenough it shouldn't be a big issue.\\n\\nMoving forward with the differed learning rate, I will pick the 10-90 validation-training split as the 0%-20% \\nvalidation set sizes did not differ significantly in accuracy. \\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Below - Training with differing validation set sizes:\n",
    "\n",
    "The accuracy of the different validation sets are as following:\n",
    "0% validation set: 85.00%\n",
    "10% validation set: 87.15%\n",
    "20% validation set: 87.48%\n",
    "30% validation set: 82.67%\n",
    "40% validation set: 83.93%\n",
    "\n",
    "One general trend between acuraccy and the size of the validation set was that as the size of \n",
    "the validation set increased, the accuracy of the trained model decreased. Doing further research\n",
    "I learned that the decrease in accuracy is likely due to the fact that there is less data for the \n",
    "model to train on when more data is allocated for validation within the full training set. The \n",
    "difference in accuracy from 0%-20% was minimal, but took a big dip from 20%-30% validation, likely\n",
    "indicating that the model underfit when 30% of the full training set was used for validation, as \n",
    "the model did not have enough data to work with.\n",
    "\n",
    "Initially, one confusing part was the rebound in accuracy at 40%, however I believe this is due to the fact that\n",
    "the training/validation split was made randomly, and therefore the model might have been trained on \n",
    "differing data that resulting in a normal fluctuation of the accuracy, although the general trend \n",
    "remains. It might be difficult to get a consistent evaluation of these due to the fact that the size \n",
    "and contents of the training set are hard to normalize due to the difference in size, although the validation\n",
    "set should be more accurate the larger the validation set is, due to there being more data used to \n",
    "validate the model during training.\n",
    "\n",
    "All in all, the size of the validation set is a balance between the risk of underfitting vs validation accuracy/\n",
    "per epoch training time. The larger the validation set, the more accurate the validation, but the smaller the training \n",
    "set, which results in shorter training time, but potentially a less accurate model. Although if the database is large\n",
    "enough it shouldn't be a big issue.\n",
    "\n",
    "Moving forward with the differed learning rate, I will pick the 10-90 validation-training split as the 0%-20% \n",
    "validation set sizes did not differ significantly in accuracy. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd70b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 60000 | Val: 0 | Test: 10000\n",
      "Epoch [1/15], Loss: 1.8668\n",
      "Epoch [2/15], Loss: 0.8099\n",
      "Epoch [3/15], Loss: 0.6262\n",
      "Epoch [4/15], Loss: 0.5419\n",
      "Epoch [5/15], Loss: 0.4858\n",
      "Epoch [6/15], Loss: 0.4446\n",
      "Epoch [7/15], Loss: 0.4199\n",
      "Epoch [8/15], Loss: 0.3936\n",
      "Epoch [9/15], Loss: 0.3750\n",
      "Epoch [10/15], Loss: 0.3632\n",
      "Epoch [11/15], Loss: 0.3464\n",
      "Epoch [12/15], Loss: 0.3341\n",
      "Epoch [13/15], Loss: 0.3209\n",
      "Epoch [14/15], Loss: 0.3091\n",
      "Epoch [15/15], Loss: 0.3042\n",
      "Accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "# 1. Trained with no validation set\n",
    "no_val_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "no_val_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(no_val_net.parameters(), lr=0.1)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=1.0)\n",
    "\n",
    "train_net(no_val_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(no_val_net, test_loader, device)\n",
    "\n",
    "torch.save(no_val_net.state_dict(), 'q2_val_train_test/no_val_net.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92094195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 2.1868, Val Acc: 48.85%\n",
      "Epoch [2/15], Loss: 1.0661, Val Acc: 71.62%\n",
      "Epoch [3/15], Loss: 0.7014, Val Acc: 73.35%\n",
      "Epoch [4/15], Loss: 0.5883, Val Acc: 75.47%\n",
      "Epoch [5/15], Loss: 0.5273, Val Acc: 80.05%\n",
      "Epoch [6/15], Loss: 0.4797, Val Acc: 82.98%\n",
      "Epoch [7/15], Loss: 0.4390, Val Acc: 83.93%\n",
      "Epoch [8/15], Loss: 0.4105, Val Acc: 85.57%\n",
      "Epoch [9/15], Loss: 0.3863, Val Acc: 85.73%\n",
      "Epoch [10/15], Loss: 0.3673, Val Acc: 84.67%\n",
      "Epoch [11/15], Loss: 0.3484, Val Acc: 86.10%\n",
      "Epoch [12/15], Loss: 0.3362, Val Acc: 82.45%\n",
      "Epoch [13/15], Loss: 0.3244, Val Acc: 87.78%\n",
      "Epoch [14/15], Loss: 0.3099, Val Acc: 87.32%\n",
      "Epoch [15/15], Loss: 0.3007, Val Acc: 88.03%\n",
      "Accuracy: 87.15%\n"
     ]
    }
   ],
   "source": [
    "# 2. Trained with 10% validation set\n",
    "ten_val_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "ten_val_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(ten_val_net.parameters(), lr=0.1)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(ten_val_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(ten_val_net, test_loader, device)\n",
    "\n",
    "torch.save(ten_val_net.state_dict(), 'q2_val_train_test/ten_val_net.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5a6c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 48000 | Val: 12000 | Test: 10000\n",
      "Epoch [1/15], Loss: 2.1865, Val Acc: 35.18%\n",
      "Epoch [2/15], Loss: 1.1363, Val Acc: 69.84%\n",
      "Epoch [3/15], Loss: 0.7202, Val Acc: 76.31%\n",
      "Epoch [4/15], Loss: 0.5965, Val Acc: 78.87%\n",
      "Epoch [5/15], Loss: 0.5308, Val Acc: 79.99%\n",
      "Epoch [6/15], Loss: 0.4901, Val Acc: 82.32%\n",
      "Epoch [7/15], Loss: 0.4520, Val Acc: 83.09%\n",
      "Epoch [8/15], Loss: 0.4254, Val Acc: 83.52%\n",
      "Epoch [9/15], Loss: 0.4021, Val Acc: 84.47%\n",
      "Epoch [10/15], Loss: 0.3833, Val Acc: 85.32%\n",
      "Epoch [11/15], Loss: 0.3683, Val Acc: 84.58%\n",
      "Epoch [12/15], Loss: 0.3528, Val Acc: 86.78%\n",
      "Epoch [13/15], Loss: 0.3416, Val Acc: 86.84%\n",
      "Epoch [14/15], Loss: 0.3238, Val Acc: 87.59%\n",
      "Epoch [15/15], Loss: 0.3162, Val Acc: 87.60%\n",
      "Accuracy: 87.48%\n"
     ]
    }
   ],
   "source": [
    "# 3. Trained with 20% validation set\n",
    "twenty_val_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "twenty_val_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(twenty_val_net.parameters(), lr=0.1)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.8)\n",
    "\n",
    "train_net(twenty_val_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(twenty_val_net, test_loader, device)\n",
    "\n",
    "torch.save(twenty_val_net.state_dict(), 'q2_val_train_test/twenty_val_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9452770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 42000 | Val: 18000 | Test: 10000\n",
      "Epoch [1/15], Loss: 2.1410, Val Acc: 34.98%\n",
      "Epoch [2/15], Loss: 1.1080, Val Acc: 45.98%\n",
      "Epoch [3/15], Loss: 0.8034, Val Acc: 47.35%\n",
      "Epoch [4/15], Loss: 0.7052, Val Acc: 63.43%\n",
      "Epoch [5/15], Loss: 0.5904, Val Acc: 60.37%\n",
      "Epoch [6/15], Loss: 0.5515, Val Acc: 60.93%\n",
      "Epoch [7/15], Loss: 0.5060, Val Acc: 69.52%\n",
      "Epoch [8/15], Loss: 0.4745, Val Acc: 44.48%\n",
      "Epoch [9/15], Loss: 0.5440, Val Acc: 64.92%\n",
      "Epoch [10/15], Loss: 0.4445, Val Acc: 81.33%\n",
      "Epoch [11/15], Loss: 0.4093, Val Acc: 64.04%\n",
      "Epoch [12/15], Loss: 0.4114, Val Acc: 73.24%\n",
      "Epoch [13/15], Loss: 0.3766, Val Acc: 83.37%\n",
      "Epoch [14/15], Loss: 0.3535, Val Acc: 80.84%\n",
      "Epoch [15/15], Loss: 0.3443, Val Acc: 83.48%\n",
      "Accuracy: 82.67%\n"
     ]
    }
   ],
   "source": [
    "# 4. Trained with 30% validation set\n",
    "thirty_val_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "thirty_val_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(thirty_val_net.parameters(), lr=0.1)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.7)\n",
    "\n",
    "train_net(thirty_val_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(thirty_val_net, test_loader, device)\n",
    "\n",
    "torch.save(thirty_val_net.state_dict(), 'q2_val_train_test/thirty_val_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be61ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 36000 | Val: 24000 | Test: 10000\n",
      "Epoch [1/15], Loss: 2.2839, Val Acc: 41.00%\n",
      "Epoch [2/15], Loss: 1.5134, Val Acc: 61.35%\n",
      "Epoch [3/15], Loss: 0.8790, Val Acc: 62.75%\n",
      "Epoch [4/15], Loss: 0.7132, Val Acc: 71.54%\n",
      "Epoch [5/15], Loss: 0.6276, Val Acc: 77.56%\n",
      "Epoch [6/15], Loss: 0.5721, Val Acc: 76.14%\n",
      "Epoch [7/15], Loss: 0.5363, Val Acc: 79.83%\n",
      "Epoch [8/15], Loss: 0.4933, Val Acc: 79.44%\n",
      "Epoch [9/15], Loss: 0.4659, Val Acc: 80.95%\n",
      "Epoch [10/15], Loss: 0.4491, Val Acc: 81.20%\n",
      "Epoch [11/15], Loss: 0.4221, Val Acc: 83.83%\n",
      "Epoch [12/15], Loss: 0.4101, Val Acc: 83.34%\n",
      "Epoch [13/15], Loss: 0.3923, Val Acc: 84.37%\n",
      "Epoch [14/15], Loss: 0.3817, Val Acc: 86.28%\n",
      "Epoch [15/15], Loss: 0.3637, Val Acc: 84.94%\n",
      "Accuracy: 83.93%\n"
     ]
    }
   ],
   "source": [
    "# 5. Trained with 40% validation set\n",
    "fourty_val_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "fourty_val_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(fourty_val_net.parameters(), lr=0.1)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.6)\n",
    "\n",
    "train_net(fourty_val_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(fourty_val_net, test_loader, device)\n",
    "\n",
    "torch.save(fourty_val_net.state_dict(), 'q2_val_train_test/fourty_val_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2d982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBelow - Training with varying SGD learning rates using 10-90 validation-training split:\\n\\n0.001 LR: 10.00%\\n0.01  LR: 73.89%\\n0.1   LR: 86.63%\\n1     LR: 10.00%\\n10    LR: 10.00%\\n\\nThe trend appears that that any learning rate too low (0.001) or too high (1, 10) will \\nresult in the model completely failing, resulting in a minimum accuracy of 10%. The \\nminimum accuracy is 10% due to the fact that the FashionMNIST is a 10-way classification,\\nso a pure guess by the model results in a 10% accuracy on average.\\n\\nThis result makes sense as a learning rate that is too low will result in the parameter barely moving, \\nas the step taken towards the negative gradient is far too small. This results in the loss of the model\\nto barely decrease and the accuracy to barely increase after each epoch. Similarly, a learning rate \\nthat is too large will result in the parameter moving too much towards the negative gradient, and even\\npotentially past the minimum cross entropy loss theta*.\\n\\nOne interesting result is that the CEL during training for the LR 10 model is NaN. This is due to the \\n-log(0) that happens when the CEL is calculated. This occurs when the model is so inaccurate that it\\npredicts the target to have a probability of 0 in the probability distribution when in reality the \\ntarget was the correct answer, resulting in the -(1*log(0) + ...) calculation. Mathematically -(1*log(0))\\nis -log(0), which is undefined or NaN.\\n\\nHere a learning rate of 0.1 had the best accuracy of 86.63%, so I will move forward with that value.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Below - Training with varying SGD learning rates using 10-90 validation-training split:\n",
    "\n",
    "0.001 LR: 10.00%\n",
    "0.01  LR: 74.11%\n",
    "0.1   LR: 86.78%\n",
    "1     LR: 10.00%\n",
    "10    LR: 10.00%\n",
    "\n",
    "The trend appears that that any learning rate too low (0.001) or too high (1, 10) will \n",
    "result in the model completely failing, resulting in a minimum accuracy of 10%. The \n",
    "minimum accuracy is 10% due to the fact that the FashionMNIST is a 10-way classification,\n",
    "so a pure guess by the model results in a 10% accuracy on average.\n",
    "\n",
    "This result makes sense as a learning rate that is too low will result in the parameter barely moving, \n",
    "as the step taken towards the negative gradient is far too small. This results in the loss of the model\n",
    "to barely decrease and the accuracy to barely increase after each epoch. Similarly, a learning rate \n",
    "that is too large will result in the parameter moving too much towards the negative gradient, and even\n",
    "potentially past the minimum cross entropy loss theta*.\n",
    "\n",
    "One interesting result is that the CEL during training for the LR 10 model is NaN. This is due to the \n",
    "-log(0) that happens when the CEL is calculated. This occurs when the model is so inaccurate that it\n",
    "predicts the target to have a probability of 0 in the probability distribution when in reality the \n",
    "target was the correct answer, resulting in the -(1*log(0) + ...) calculation. Mathematically -(1*log(0))\n",
    "is -log(0), which is undefined or NaN.\n",
    "\n",
    "Here a learning rate of 0.1 had the best accuracy of 86.63%, so I will move forward with that value.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ef5a14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 2.3030, Val Acc: 10.05%\n",
      "Epoch [2/15], Loss: 2.3027, Val Acc: 10.05%\n",
      "Epoch [3/15], Loss: 2.3025, Val Acc: 10.05%\n",
      "Epoch [4/15], Loss: 2.3022, Val Acc: 10.05%\n",
      "Epoch [5/15], Loss: 2.3019, Val Acc: 10.05%\n",
      "Epoch [6/15], Loss: 2.3016, Val Acc: 10.05%\n",
      "Epoch [7/15], Loss: 2.3014, Val Acc: 10.05%\n",
      "Epoch [8/15], Loss: 2.3011, Val Acc: 10.05%\n",
      "Epoch [9/15], Loss: 2.3009, Val Acc: 10.05%\n",
      "Epoch [10/15], Loss: 2.3006, Val Acc: 10.05%\n",
      "Epoch [11/15], Loss: 2.3003, Val Acc: 10.05%\n",
      "Epoch [12/15], Loss: 2.3000, Val Acc: 10.05%\n",
      "Epoch [13/15], Loss: 2.2998, Val Acc: 10.05%\n",
      "Epoch [14/15], Loss: 2.2995, Val Acc: 10.05%\n",
      "Epoch [15/15], Loss: 2.2992, Val Acc: 10.05%\n",
      "Accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# 1. Trained with 0.001 Learning Rate\n",
    "point_o_o_one_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "point_o_o_one_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (0.001 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(point_o_o_one_net.parameters(), lr=0.001)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(point_o_o_one_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(point_o_o_one_net, test_loader, device)\n",
    "\n",
    "torch.save(point_o_o_one_net.state_dict(), 'q3_learning_rate_test/point_o_o_one_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35076f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 2.3028, Val Acc: 10.42%\n",
      "Epoch [2/15], Loss: 2.2996, Val Acc: 10.42%\n",
      "Epoch [3/15], Loss: 2.2962, Val Acc: 11.32%\n",
      "Epoch [4/15], Loss: 2.2918, Val Acc: 22.05%\n",
      "Epoch [5/15], Loss: 2.2847, Val Acc: 29.57%\n",
      "Epoch [6/15], Loss: 2.2717, Val Acc: 49.15%\n",
      "Epoch [7/15], Loss: 2.2396, Val Acc: 42.30%\n",
      "Epoch [8/15], Loss: 2.1038, Val Acc: 43.83%\n",
      "Epoch [9/15], Loss: 1.4761, Val Acc: 57.53%\n",
      "Epoch [10/15], Loss: 1.0358, Val Acc: 66.03%\n",
      "Epoch [11/15], Loss: 0.8905, Val Acc: 63.67%\n",
      "Epoch [12/15], Loss: 0.8015, Val Acc: 67.48%\n",
      "Epoch [13/15], Loss: 0.7541, Val Acc: 68.13%\n",
      "Epoch [14/15], Loss: 0.7082, Val Acc: 71.82%\n",
      "Epoch [15/15], Loss: 0.6752, Val Acc: 75.03%\n",
      "Accuracy: 74.11%\n"
     ]
    }
   ],
   "source": [
    "# 2. Trained with 0.01 Learning Rate\n",
    "point_o_one_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "point_o_one_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (0.001 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(point_o_one_net.parameters(), lr=0.01)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(point_o_one_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(point_o_one_net, test_loader, device)\n",
    "\n",
    "torch.save(point_o_one_net.state_dict(), 'q3_learning_rate_test/point_o_one_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd92f4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 2.0065, Val Acc: 40.28%\n",
      "Epoch [2/15], Loss: 0.9320, Val Acc: 74.72%\n",
      "Epoch [3/15], Loss: 0.6597, Val Acc: 75.85%\n",
      "Epoch [4/15], Loss: 0.5552, Val Acc: 73.90%\n",
      "Epoch [5/15], Loss: 0.4968, Val Acc: 81.77%\n",
      "Epoch [6/15], Loss: 0.4536, Val Acc: 83.03%\n",
      "Epoch [7/15], Loss: 0.4197, Val Acc: 84.47%\n",
      "Epoch [8/15], Loss: 0.3915, Val Acc: 84.83%\n",
      "Epoch [9/15], Loss: 0.3667, Val Acc: 84.65%\n",
      "Epoch [10/15], Loss: 0.3549, Val Acc: 84.70%\n",
      "Epoch [11/15], Loss: 0.3357, Val Acc: 84.42%\n",
      "Epoch [12/15], Loss: 0.3269, Val Acc: 86.42%\n",
      "Epoch [13/15], Loss: 0.3115, Val Acc: 87.78%\n",
      "Epoch [14/15], Loss: 0.3055, Val Acc: 87.35%\n",
      "Epoch [15/15], Loss: 0.2971, Val Acc: 87.35%\n",
      "Accuracy: 86.78%\n"
     ]
    }
   ],
   "source": [
    "# 3. Trained with 0.1 Learning Rate\n",
    "point_one_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "point_one_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (0.01 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(point_one_net.parameters(), lr=0.1)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(point_one_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(point_one_net, test_loader, device)\n",
    "\n",
    "torch.save(point_one_net.state_dict(), 'q3_learning_rate_test/point_one_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3e09a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 2.4207, Val Acc: 9.92%\n",
      "Epoch [2/15], Loss: 2.2105, Val Acc: 9.92%\n",
      "Epoch [3/15], Loss: 2.2850, Val Acc: 10.08%\n",
      "Epoch [4/15], Loss: 2.3041, Val Acc: 9.60%\n",
      "Epoch [5/15], Loss: 2.3037, Val Acc: 9.60%\n",
      "Epoch [6/15], Loss: 2.3037, Val Acc: 9.60%\n",
      "Epoch [7/15], Loss: 2.3038, Val Acc: 9.90%\n",
      "Epoch [8/15], Loss: 2.3036, Val Acc: 10.13%\n",
      "Epoch [9/15], Loss: 2.3034, Val Acc: 10.35%\n",
      "Epoch [10/15], Loss: 2.3037, Val Acc: 9.90%\n",
      "Epoch [11/15], Loss: 2.3035, Val Acc: 9.60%\n",
      "Epoch [12/15], Loss: 2.3036, Val Acc: 9.92%\n",
      "Epoch [13/15], Loss: 2.3035, Val Acc: 10.35%\n",
      "Epoch [14/15], Loss: 2.3035, Val Acc: 9.90%\n",
      "Epoch [15/15], Loss: 2.3034, Val Acc: 10.08%\n",
      "Accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# 4. Trained with 1.0 Learning Rate\n",
    "one_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "one_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (1 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(one_net.parameters(), lr=1)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(one_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(one_net, test_loader, device)\n",
    "\n",
    "torch.save(one_net.state_dict(), 'q3_learning_rate_test/one_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79801b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [2/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [3/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [4/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [5/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [6/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [7/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [8/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [9/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [10/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [11/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [12/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [13/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [14/15], Loss: nan, Val Acc: 9.82%\n",
      "Epoch [15/15], Loss: nan, Val Acc: 9.82%\n",
      "Accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# 5. Trained with 10 Learning Rate\n",
    "ten_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "ten_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (10 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(ten_net.parameters(), lr=10)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(ten_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(ten_net, test_loader, device)\n",
    "\n",
    "torch.save(ten_net.state_dict(), 'q3_learning_rate_test/ten_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773f8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nBelow - Training with Adam Algorithm\\n\\nLR 0.1: 10.00%\\nLR 0.01: 90.98%\\nLR 0.001: 91.24%\\nLR 0.0001: 85.57% \\n\\nUsing the best learning rate from the SGD algorithm (0.1), the Adam algorithm performed poorly. The model \\nwas at the minimum probability accuracy of 10%. This was confusing, but after some research, I learned \\nthat the adam algorithm has different requirements for learning rates, and unlike the SGD algorithm, Adam \\ntypically performs better with a learning rate between 0.001~0.01. Therefore I also ran extra tests with \\nthose learning rates.\\n\\nSurprisingly, when using a smaller learning rate, the accuracy of the model using those learning rates \\nand the adam algorithm were much better than the models trained with SGD, with a maximum accuracy being \\nachieved with a learning rate of 0.001, at 91.24%. However the accuracy of the model decreased when a \\nlearning rate of 1e-4 was used.\\n\\nOne additional finding was that during the 0.01 and 0.001 learning rate training, the accuracy of the \\nmodel seemed to plateau at around around 90% after 6-7 epochs, which may be a performance ceiling as \\nwith the current setup of the project.\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Below - Training with Adam Algorithm\n",
    "\n",
    "LR 0.1: 10.00%\n",
    "LR 0.01: 91.26%\n",
    "LR 0.001: 91.24%\n",
    "LR 0.0001: 85.16% \n",
    "\n",
    "Using the best learning rate from the SGD algorithm (0.1), the Adam algorithm performed poorly. The model \n",
    "was at the minimum probability accuracy of 10%. This was confusing, but after some research, I learned \n",
    "that the adam algorithm has different requirements for learning rates, and unlike the SGD algorithm, Adam \n",
    "typically performs better with a learning rate between 0.001~0.01. Therefore I also ran extra tests with \n",
    "those learning rates.\n",
    "\n",
    "Surprisingly, when using a smaller learning rate, the accuracy of the model using those learning rates \n",
    "and the adam algorithm were much better than the models trained with SGD, with a maximum accuracy being \n",
    "achieved with a learning rate of 0.01, at 91.26%. However the accuracy of the model decreased when a \n",
    "learning rate of 1e-4 was used.\n",
    "\n",
    "One additional finding was that during the 0.01 and 0.001 learning rate training, the accuracy of the \n",
    "model seemed to plateau at around around 90% after 6-7 epochs, which may be a performance ceiling as \n",
    "with the current setup of the project.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c990657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 413.6102, Val Acc: 10.23%\n",
      "Epoch [2/15], Loss: 2.3049, Val Acc: 9.50%\n",
      "Epoch [3/15], Loss: 2.3053, Val Acc: 10.23%\n",
      "Epoch [4/15], Loss: 2.3046, Val Acc: 10.35%\n",
      "Epoch [5/15], Loss: 2.3056, Val Acc: 10.38%\n",
      "Epoch [6/15], Loss: 2.3052, Val Acc: 9.97%\n",
      "Epoch [7/15], Loss: 2.3051, Val Acc: 10.38%\n",
      "Epoch [8/15], Loss: 2.3050, Val Acc: 10.02%\n",
      "Epoch [9/15], Loss: 2.3057, Val Acc: 10.35%\n",
      "Epoch [10/15], Loss: 2.3053, Val Acc: 10.23%\n",
      "Epoch [11/15], Loss: 2.3053, Val Acc: 9.52%\n",
      "Epoch [12/15], Loss: 2.3055, Val Acc: 9.52%\n",
      "Epoch [13/15], Loss: 2.3057, Val Acc: 9.50%\n",
      "Epoch [14/15], Loss: 2.3053, Val Acc: 10.23%\n",
      "Epoch [15/15], Loss: 2.3065, Val Acc: 10.38%\n",
      "Accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# Trained with Adam\n",
    "adam_net = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "adam_net.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (0.1 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(adam_net.parameters(), lr=0.1)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(adam_net, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(adam_net, test_loader, device)\n",
    "\n",
    "torch.save(adam_net.state_dict(), 'q4_adam_test/adam_net.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45adbc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 0.7722, Val Acc: 84.75%\n",
      "Epoch [2/15], Loss: 0.3665, Val Acc: 87.55%\n",
      "Epoch [3/15], Loss: 0.3001, Val Acc: 90.15%\n",
      "Epoch [4/15], Loss: 0.2712, Val Acc: 90.57%\n",
      "Epoch [5/15], Loss: 0.2515, Val Acc: 90.80%\n",
      "Epoch [6/15], Loss: 0.2372, Val Acc: 91.07%\n",
      "Epoch [7/15], Loss: 0.2272, Val Acc: 91.03%\n",
      "Epoch [8/15], Loss: 0.2125, Val Acc: 91.48%\n",
      "Epoch [9/15], Loss: 0.1999, Val Acc: 92.27%\n",
      "Epoch [10/15], Loss: 0.1932, Val Acc: 91.28%\n",
      "Epoch [11/15], Loss: 0.1824, Val Acc: 91.73%\n",
      "Epoch [12/15], Loss: 0.1764, Val Acc: 91.25%\n",
      "Epoch [13/15], Loss: 0.1779, Val Acc: 91.77%\n",
      "Epoch [14/15], Loss: 0.1745, Val Acc: 92.23%\n",
      "Epoch [15/15], Loss: 0.1664, Val Acc: 92.07%\n",
      "Accuracy: 91.26%\n"
     ]
    }
   ],
   "source": [
    "# Trained with Adam 1e-2 learning rate\n",
    "adam_net_o_one = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "adam_net_o_one.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (0.001 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(adam_net_o_one.parameters(), lr=1e-2)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(adam_net_o_one, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(adam_net_o_one, test_loader, device)\n",
    "\n",
    "torch.save(adam_net_o_one.state_dict(), 'q4_adam_test/adam_net_o_one.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7eb5ee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 0.8743, Val Acc: 78.30%\n",
      "Epoch [2/15], Loss: 0.4730, Val Acc: 83.58%\n",
      "Epoch [3/15], Loss: 0.3811, Val Acc: 87.07%\n",
      "Epoch [4/15], Loss: 0.3358, Val Acc: 87.75%\n",
      "Epoch [5/15], Loss: 0.3034, Val Acc: 88.82%\n",
      "Epoch [6/15], Loss: 0.2773, Val Acc: 89.38%\n",
      "Epoch [7/15], Loss: 0.2608, Val Acc: 89.73%\n",
      "Epoch [8/15], Loss: 0.2475, Val Acc: 89.67%\n",
      "Epoch [9/15], Loss: 0.2313, Val Acc: 90.07%\n",
      "Epoch [10/15], Loss: 0.2241, Val Acc: 91.23%\n",
      "Epoch [11/15], Loss: 0.2098, Val Acc: 91.55%\n",
      "Epoch [12/15], Loss: 0.1987, Val Acc: 91.23%\n",
      "Epoch [13/15], Loss: 0.1877, Val Acc: 91.42%\n",
      "Epoch [14/15], Loss: 0.1775, Val Acc: 91.57%\n",
      "Epoch [15/15], Loss: 0.1747, Val Acc: 91.33%\n",
      "Accuracy: 91.24%\n"
     ]
    }
   ],
   "source": [
    "# Trained with Adam 1e-3 learning rate\n",
    "adam_net_o_o_one = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "adam_net_o_o_one.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (0.001 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(adam_net_o_o_one.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(adam_net_o_o_one, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(adam_net_o_o_one, test_loader, device)\n",
    "\n",
    "torch.save(adam_net_o_o_one.state_dict(), 'q4_adam_test/adam_net_o_o_one.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8698568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 54000 | Val: 6000 | Test: 10000\n",
      "Epoch [1/15], Loss: 1.6981, Val Acc: 67.80%\n",
      "Epoch [2/15], Loss: 0.7749, Val Acc: 74.50%\n",
      "Epoch [3/15], Loss: 0.6513, Val Acc: 77.12%\n",
      "Epoch [4/15], Loss: 0.5922, Val Acc: 78.05%\n",
      "Epoch [5/15], Loss: 0.5553, Val Acc: 79.35%\n",
      "Epoch [6/15], Loss: 0.5283, Val Acc: 81.28%\n",
      "Epoch [7/15], Loss: 0.5074, Val Acc: 82.07%\n",
      "Epoch [8/15], Loss: 0.4838, Val Acc: 82.68%\n",
      "Epoch [9/15], Loss: 0.4675, Val Acc: 83.32%\n",
      "Epoch [10/15], Loss: 0.4517, Val Acc: 84.17%\n",
      "Epoch [11/15], Loss: 0.4399, Val Acc: 84.08%\n",
      "Epoch [12/15], Loss: 0.4244, Val Acc: 84.88%\n",
      "Epoch [13/15], Loss: 0.4120, Val Acc: 85.20%\n",
      "Epoch [14/15], Loss: 0.3994, Val Acc: 85.58%\n",
      "Epoch [15/15], Loss: 0.3914, Val Acc: 86.07%\n",
      "Accuracy: 85.16%\n"
     ]
    }
   ],
   "source": [
    "# Trained with Adam 1e-4 learning rate\n",
    "adam_net_o_o_o_one = FashionMNISTNet()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "adam_net_o_o_o_one.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer (0.001 LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(adam_net_o_o_o_one.parameters(), lr=1e-4)\n",
    "\n",
    "train_loader, val_loader, test_loader = train_val_split(train_ratio=0.9)\n",
    "\n",
    "train_net(adam_net_o_o_o_one, train_loader, val_loader, device, criterion, optimizer)\n",
    "\n",
    "evaluate_model(adam_net_o_o_o_one, test_loader, device)\n",
    "\n",
    "torch.save(adam_net_o_o_o_one.state_dict(), 'q4_adam_test/adam_net_o_o_o_one.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0755b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBelow - Area Under Curve Testing \\n\\nusing the best model: 10-90 validation-training split, Adam algorithm, 0.001 LR.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Below - Area Under Curve Testing \n",
    "\n",
    "using the best model: 10-90 validation-training split, Adam algorithm, 0.01 LR.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "248f6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score #computes false-positive rates and true positive rates\n",
    "import torch.nn.functional as F \n",
    "\n",
    "def evaluate_model_auc(model, test_loader, device, positive_class):\n",
    "        model.eval()\n",
    "\n",
    "        #Test\n",
    "        y_true = [] #holds 0/1 ground truth values\n",
    "        y_score = [] #holds predicted probability scores for class 2\n",
    "\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "\n",
    "                        # Move the inputs and labels to the GPU if available\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        # Forward pass\n",
    "                        outputs = model(inputs)\n",
    "                        probabilities = F.softmax(outputs, dim=1) #converts outputs into probabilities\n",
    "\n",
    "                        # Build binary lists\n",
    "                        y_true.extend((labels == positive_class).int().cpu().numpy())\n",
    "                        y_score.extend(probabilities[:, positive_class].cpu().numpy())\n",
    "\n",
    "        # Compute ROC curve\n",
    "        auc_val = roc_auc_score(y_true, y_score)\n",
    "\n",
    "        # return results\n",
    "        print(f\"AUC: {auc_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
